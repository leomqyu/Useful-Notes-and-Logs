{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da42d4b",
   "metadata": {},
   "source": [
    "# 1. pytorch native amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5083319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pytorch native amp:\n",
    "    source: https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html \n",
    "    -- auto mp: some op use float16 or bfloat16, some still 32 bits\n",
    "\n",
    "Explanation:\n",
    "0. Without amp/autocast ..., the default precision is 32 bits\n",
    "1. Inside the `torch.autocast`, will do amp that automatically cast some operation to less bits\n",
    "2. Usually used with gradient scalor that helps prevent gradients with small magnitudes from \n",
    "    flushing to zero (“underflowing”) when training with mixed precision.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "use_amp = True\n",
    "model = make_model()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "scaler = torch.amp.GradScaler(\"cuda\" ,enabled=use_amp)  # can also use other scalor, eg from timm\n",
    "\n",
    "for epoch in range(0): \n",
    "    for input, target in zip(data, targets):\n",
    "\n",
    "        # (1) `device_type` param must be specified for `torch.autocastm` but not for `torch.cuda.amp.autocast`\n",
    "        # (2) dtype param is the the target low-precision dtype; default is None, then go to device’s default autocast dtype\n",
    "        # (3) No functional diff between `torch.autocast` (new, work on GPU, CPU, ...) \n",
    "        #       and `torch.cuda.amp.autocast` (old, CUDA-only) -> then don't need to specify device type\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n",
    "            output = model(input)   # (usually?) output.dtype is torch.float16 \n",
    "            loss = loss_fn(output, target)  # (usually?) loss.dtype is torch.float32  \n",
    "\n",
    "        # Backward passes under autocast are not recommended.\n",
    "        # Scales loss. Calls ``backward()`` on scaled loss to create scaled gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "        # ``scaler.step()`` first unscales the gradients of the optimizer's assigned parameters.\n",
    "        # If these gradients do not contain ``inf``s or ``NaN``s, optimizer.step() is then called,\n",
    "        # otherwise, optimizer.step() is skipped.\n",
    "        scaler.step(optimizer)\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50370cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Another eg:\n",
    "    `with torch.autocast(enabled=False)` doesn't mean skip operation, but means turn off\n",
    "\"\"\"\n",
    "\n",
    "with torch.autocast(enabled=True):\n",
    "    'op1'   # with autocast\n",
    "    with torch.autocast(enabled=False):\n",
    "        'op2' # without autocast\n",
    "    'op3'   # with autocast\n",
    "'op4'   # without autocast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47af1d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "technotes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
