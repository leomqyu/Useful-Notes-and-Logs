{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16891ee3",
   "metadata": {},
   "source": [
    "# Difference between different torch.nn norm methods\n",
    "LayerNorm, BatchNorm (1d, 2d, 3d)  \n",
    "-- [new note ?] Only need to remember nn.LayerNorm, and all the other norm can be realized with einops + this nn.LayerNorm\n",
    "-- [?] when we say normalize across some dimension, means that loop through the combination of other dimensions, and for each fixed combination, the rest of the \"flexible\" target dimension values, normalize them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd883ce1",
   "metadata": {},
   "source": [
    "# 1. torch.nn.Layernorm\n",
    "\n",
    "1. [documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)\n",
    "1. The idea of layer norm is to normalize all the features (instead of batches), i.e. normalize over the feature dimension.    \n",
    "    but how does the code know which dimension is the feature? By you telling the model in the parameter and by some model conventions.\n",
    "1. Note, also for all other norms, this norm has learnable affine transform parameters by default. Check documentation for details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fcb759",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The rule is complex, but CCLD for common usage:\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "nn.LayerNorm(512)               # Normalize last dimension of size 512: most common\n",
    "nn.LayerNorm([100, 512])        # Normalize last 2 dimensions of the shape argument\n",
    "nn.LayerNorm([50, 100, 512])    # Normalize last 3 dimensions of the shape argument\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d537f6b",
   "metadata": {},
   "source": [
    "# torch.nn batch norm family\n",
    "\n",
    "1. The idea is to norm over the batch dimension.  \n",
    "    But how does the code know which dimension is the feature? By you telling the model in the parameter and by some model conventions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713c473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. BatchNorm1d:\n",
    "    - Expects: [batch_size, num_features] or [batch_size, num_features, length]\n",
    "    - BatchNorm1d normalizes across dim=0 (batch) for each feature channel (note: if has length, not normalizing all lengths!)\n",
    "    - It assumes features are in dim=1\n",
    "\"\"\"\n",
    "# initialize\n",
    "num_features = 100\n",
    "m = nn.BatchNorm1d(num_features)\n",
    "\n",
    "# calculate batch norm\n",
    "input = torch.randn(20, 100)\n",
    "output = m(input)\n",
    "\n",
    "\"\"\"\n",
    "2. BatchNorm2d:\n",
    "    - Expects: [batch_size, num_channels, height, width]\n",
    "    - BatchNorm2d normalizes across dim=0 (batch) for each channel\n",
    "    - It assumes channels are in dim=1\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "3. BatchNorm3d:\n",
    "    - Expects: [batch_size, num_channels, depth, height, width]\n",
    "    - BatchNorm3d normalizes across dim=0 (batch) for each channel\n",
    "    - It assumes channels are in dim=1\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "technotes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
