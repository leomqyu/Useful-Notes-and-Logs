{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Idea: \n",
    "0. first used in CNN.  \n",
    "1. When model is trained, its params fixed, and let the activation map (eg at the last layer) as the variables. \n",
    "   Do backward propagation to these activations and get a gradient map\n",
    "2. Do mean on the gradient of these activation maps for different channels, do relu, resize it to be the same size as image. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e3280",
   "metadata": {},
   "source": [
    "![alt text](<grad_cam_illustration.png>)\n",
    "\n",
    "from: https://xai-tutorials.readthedocs.io/en/latest/_model_specific_xai/Grad-CAM.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a959b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Eg for CNN, from: https://medium.com/@bmuskan007/grad-cam-a-beginners-guide-adf68e80f4bb\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary packages and libraries\n",
    "import torchvision\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load pre-trained model\n",
    "vgg_model = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "# transformation for passing image into the network\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "MOST IMPORTANT\n",
    "\"\"\"\n",
    "# selecting layers from the model to generate activations\n",
    "image_to_heatmaps = nn.Sequential(*list(vgg_model.features[:-4]))       # calculate until 4th from last layer -- get activation map \n",
    "def compute_heatmap(model,img):\n",
    "  model.eval()\n",
    "\n",
    "  \"\"\"get label\"\"\"\n",
    "  # compute logits from the model\n",
    "  logits = model(img)\n",
    "  # model's prediction \n",
    "  pred = logits.max(-1)[-1]\n",
    "\n",
    "  \"\"\"get activation\"\"\"\n",
    "  # activations from the model\n",
    "  activations = image_to_heatmaps(img)\n",
    "\n",
    "  \"\"\"get gradient\"\"\"\n",
    "  # compute gradients with respect to the model's most confident prediction\n",
    "  logits[0, pred].backward(retain_graph=True)   # when do this, the gradient for each param of the model will be calculated at the param\n",
    "  # average gradients of the featuremap \n",
    "  pool_grads = model.features[-3].weight.grad.data.mean((0,2,3))\n",
    "  # multiply each activation map with corresponding gradient average\n",
    "  for i in range(activations.shape[1]):\n",
    "    activations[:,i,:,:] *= pool_grads[i]\n",
    "  # calculate mean of weighted activations\n",
    "  heatmap = torch.mean(activations, dim=1)[0].cpu().detach()\n",
    "  return heatmap, pred\n",
    "\n",
    "\n",
    "def upsampleHeatmap(map, image):\n",
    "  # permute image\n",
    "  image = image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "  # maximum and minimum value from heatmap\n",
    "  m, M = map.min(), map.max()\n",
    "  # normalize the heatmap\n",
    "  map = 255 * ((map-m)/ (m-M))\n",
    "  map = np.uint8(map)\n",
    "  # resize the heatmap to the same as the input\n",
    "  map = cv2.resize(map, (224, 224))\n",
    "  map = cv2.applyColorMap(255-map, cv2.COLORMAP_JET)\n",
    "  map = np.uint8(map)\n",
    "  # change this to balance between heatmap and image\n",
    "  map = np.uint8(map*0.7 + image*0.3)\n",
    "  return map\n",
    "\n",
    "def display_images(upsampled_map, image):\n",
    "    image = image.squeeze(0).permute(1, 2, 0)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    axes[0].imshow(upsampled_map)\n",
    "    axes[0].set_title(\"Heatmap\")\n",
    "    axes[0].axis('off')\n",
    "    axes[1].imshow(image)\n",
    "    axes[1].set_title(\"Original Image\")\n",
    "    axes[1].axis('off')\n",
    "    plt.show()\n",
    "# Example usage\n",
    "cat_dog_img = \"cat_and_dog.jpg\"\n",
    "cat_dog_img = Image.open(cat_dog_img)\n",
    "cat_dog_img = transform(cat_dog_img)\n",
    "\n",
    "cat_dog_img = cat_dog_img.unsqueeze(0)\n",
    "heatmap,pred = compute_heatmap(vgg_model,cat_dog_img)\n",
    "upsampled_map = upsampleHeatmap(heatmap, cat_dog_img)\n",
    "print(f\"Prediction: {pred}\")\n",
    "\n",
    "display_images(upsampled_map, cat_dog_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff220fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
